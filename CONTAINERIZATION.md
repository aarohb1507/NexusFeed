# NexusFeed Containerization Architecture

## Overview
This document outlines the containerization design for NexusFeed microservices, including Docker image strategy, container networking, and service communication patterns.

---

## 1. Image Build Strategy

### Multi-Stage Build Philosophy
Each service uses a **two-stage Dockerfile**:
- **Stage 1 (Builder)**: Installs all dependencies (including devDependencies)
- **Stage 2 (Runtime)**: Copies only production dependencies and source code

**Benefits**:
- Smaller final image size (~150-180MB vs ~400MB)
- Reduced attack surface (no build tools in production)
- Consistent base image across all services
- Faster deployment pulls

### Base Image
- **Image**: `node:18-alpine`
- **Why Alpine**: 
  - 40MB base vs 300MB+ full Node
  - Security patches applied regularly
  - Lightweight for microservices

### Layer Caching Strategy
```
Layer 1: Alpine base (cached across all services)
Layer 2: npm dependencies (cached per service if package.json unchanged)
Layer 3: Application code (changes frequently, good isolation)
```

---

## 2. Service Image Architecture

### Per-Service Dockerfiles (5 total)
Each service gets an identical Dockerfile template with service-specific port exposure:

```
â”œâ”€â”€ api-gateway
â”‚   â””â”€â”€ Dockerfile (EXPOSE 3000)
â”œâ”€â”€ identity-service
â”‚   â””â”€â”€ Dockerfile (EXPOSE 3001)
â”œâ”€â”€ post-service
â”‚   â””â”€â”€ Dockerfile (EXPOSE 3002)
â”œâ”€â”€ media-service
â”‚   â””â”€â”€ Dockerfile (EXPOSE 3003)
â””â”€â”€ search-service
    â””â”€â”€ Dockerfile (EXPOSE 3004)
```

### Key Dockerfile Decisions

| Decision | Value | Rationale |
|----------|-------|-----------|
| **User Privilege** | `node:node` (non-root) | Security best practice |
| **Port Exposure** | Individual per service | Network isolation design |
| **Health Checks** | Included | Container orchestration readiness |
| **Environment** | Node.js production | Security, performance |
| **Logging** | STDOUT/STDERR | Docker logs integration |

---

## 3. Container Networking Architecture

### Network Type
- **Driver**: `bridge` (default custom bridge)
- **Name**: `nexusfeed-network`
- **Isolation**: Services communicate via container names (internal DNS)

### Service-to-Service Communication

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       nexusfeed-network (bridge)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                             â”‚
â”‚  api-gateway (3000) â”€â”€â”                     â”‚
â”‚                       â”œâ”€â”€> identity-service â”‚
â”‚                       â”œâ”€â”€> post-service     â”‚
â”‚                       â”œâ”€â”€> media-service    â”‚
â”‚                       â””â”€â”€> search-service   â”‚
â”‚                                             â”‚
â”‚  RabbitMQ (5672) â—„â”€â”€â”€â”€â”¬â”€â”€â”€ post-service    â”‚
â”‚                       â”œâ”€â”€â”€ media-service   â”‚
â”‚                       â””â”€â”€â”€ search-service  â”‚
â”‚                                             â”‚
â”‚  MongoDB (27017) â—„â”€â”€â”€â”€â”¬â”€â”€â”€ all services    â”‚
â”‚  Redis (6379) â—„â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€ gateway/all     â”‚
â”‚                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### DNS Resolution
- Services discover each other via container name (e.g., `http://post-service:3002`)
- Docker's embedded DNS resolver handles `service-name:port` lookups
- No external service discovery needed for this scale

### Network Policies
- **Ingress**: Only API Gateway (3000) exposed to host
- **Internal**: All service-to-service traffic on bridge
- **Isolation**: Services cannot talk to external IPs unless explicitly configured

---

## 4. Service Communication Contracts

### Request Flow (HTTP/REST)
```
Client Request
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   API Gateway (3000)        â”‚ Rate limiting, auth validation
â”‚   (api-gateway-container)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                 â”‚                  â”‚
    â–¼                 â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Identity Svc â”‚ â”‚ Post Svc     â”‚ â”‚ Media Svc    â”‚
â”‚ (3001)       â”‚ â”‚ (3002)       â”‚ â”‚ (3003)       â”‚
â”‚ jwt validate â”‚ â”‚ cache layer  â”‚ â”‚ file upload  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚                 â”‚                  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
          RabbitMQ (5672)
       (async event queue)
                 â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚            â”‚            â”‚
    â–¼            â–¼            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Media   â”‚ â”‚ Search   â”‚ â”‚ Cache    â”‚
â”‚ Handler â”‚ â”‚ Handler  â”‚ â”‚ Invalidation
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚            â”‚
    â–¼            â–¼
 MongoDB    Denormalized
            Search Index
```

### Synchronous Communication (Gateway â†’ Services)
- **Protocol**: HTTP/REST via proxy
- **Timeout**: 30s (configurable)
- **Error Handling**: 
  - 5xx errors â†’ return to client
  - Connection timeout â†’ circuit breaker pattern
- **Port Mapping**: 
  - Gateway receives on :3000
  - Routes to internal service addresses
  - Example: `/v1/posts/*` â†’ `http://post-service:3002/api/posts/*`

### Asynchronous Communication (Event-Driven)
- **Protocol**: RabbitMQ AMQP
- **Exchange**: `facebook_events` (topic-based)
- **Message Format**: JSON with routing keys

**Event Types**:
```
post.created      â†’ post-service publishes
                     â”œâ”€â†’ search-service listens (indexing)
                     â””â”€â†’ media-service listens (metadata link)

post.deleted      â†’ post-service publishes
                     â”œâ”€â†’ search-service listens (index cleanup)
                     â””â”€â†’ media-service listens (media cleanup)

media.uploaded    â†’ media-service publishes
                     â””â”€â†’ post-service listens (associate media)

search.indexed    â†’ search-service publishes
                     â””â”€â†’ post-service listens (cache update)
```

### Data Layer Communication

**MongoDB**:
- Each service connects to shared MongoDB instance
- Connection string: `mongodb://mongo:27017/nexusfeed`
- Each service may have its own database or collection namespace

**Redis**:
- API Gateway: Rate limiting storage
- Post Service: Query result caching
- Search Service: Result caching
- Connection: `redis://redis:6379`

---

## 5. Docker Compose Architecture

### Service Orchestration

```yaml
version: '3.9'
services:
  # Infrastructure Tier
  mongo:        # Database (port 27017)
  redis:        # Cache (port 6379)
  rabbitmq:     # Message broker (port 5672)
  
  # Application Tier
  api-gateway:      # Port 3000 (exposed to host)
  identity-service: # Port 3001 (internal only)
  post-service:     # Port 3002 (internal only)
  media-service:    # Port 3003 (internal only)
  search-service:   # Port 3004 (internal only)

volumes:
  mongo-data:    # Persistent database
  redis-data:    # Persistent cache
  rabbitmq-data: # Persistent queues

networks:
  nexusfeed-network: # Single bridge network for all services
```

### Container Startup Order & Dependencies

**Dependency Chain** (using `depends_on: condition`):
```
1. mongo (MongoDB) â€” readiness: mongosh -eval "db.adminCommand('ping')"
2. redis (Redis) â€” readiness: redis-cli ping
3. rabbitmq (RabbitMQ) â€” readiness: rabbitmqctl status

4. identity-service â€” depends on: mongo
5. post-service â€” depends on: mongo, redis, rabbitmq
6. media-service â€” depends on: mongo, rabbitmq
7. search-service â€” depends on: mongo, redis, rabbitmq

8. api-gateway â€” depends on: all services (last)
```

**Rationale**: Infrastructure must be ready before services; services before gateway.

### Environment Injection

Services receive environment variables from `.env.docker`:
```
# Internal service URLs (within bridge network)
IDENTITY_SERVICE_URL=http://identity-service:3001
POST_SERVICE_URL=http://post-service:3002
MEDIA_SERVICE_URL=http://media-service:3003
SEARCH_SERVICE_URL=http://search-service:3004

# Database URLs (internal container DNS)
MONGODB_URI=mongodb://mongo:27017/nexusfeed
REDIS_URL=redis://redis:6379

# Message Queue
RABBITMQ_URL=amqp://guest:guest@rabbitmq:5672
```

---

## 6. Port Exposure Strategy

### Host Port Mapping
```
Host:Container
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
3000:3000   â†’ API Gateway (only public-facing port)

(Internally on bridge network, all others accessible by name)
```

### Rationale
- **Single Exposure Point**: Only API Gateway to host
- **Service Discovery**: Other services found via container DNS
- **Security**: No direct container port exposure for internal services
- **Load Balancing**: Ready for reverse proxy or load balancer in front of 3000

---

## 7. Volume Management

### Persistence Strategy

| Service | Volume | Purpose | Lifetime |
|---------|--------|---------|----------|
| MongoDB | `mongo-data` | User + Post data | Persistent across restarts |
| Redis | `redis-data` | Cache + Rate limits | Can be ephemeral, but persisted for consistency |
| RabbitMQ | `rabbitmq-data` | Message queue | Persistent to prevent message loss |
| Services | None | Stateless containers | Recreated on restart |

### Volume Drivers
- **Type**: Named volumes (managed by Docker)
- **Location**: `/var/lib/docker/volumes/` on host
- **Backup**: Standard Docker backup/restore procedures apply

---

## 8. Health Checks & Liveness

### Container Health Checks
Each service Dockerfile includes:
```
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3
  CMD curl -f http://localhost:PORT/health || exit 1
```

**Health Endpoint Contract**:
```
GET /health
Response: { "status": "ok", "service": "post-service" }
HTTP 200
```

### Docker Compose Health Monitoring
Compose file specifies:
- Interval: 30 seconds
- Timeout: 10 seconds
- Start period: 5 seconds (grace before checking)
- Retries: 3 failures before restart

---

## 9. Build & Push Workflow

### Local Development
```bash
docker-compose build --no-cache              # Rebuild all images
docker-compose up -d                         # Start all containers
docker-compose logs -f api-gateway          # Watch logs
docker-compose down -v                      # Cleanup + volumes
```

### CI/CD Pipeline (Future)
```
Trigger: Push to main branch
  1. Build images with tag: nexusfeed:SERVICE:$VERSION
  2. Scan for vulnerabilities: trivy scan
  3. Push to registry (Docker Hub / ECR / private registry)
  4. Update docker-compose.yml with new tag versions
  5. Deploy to staging/production
```

### Image Naming Convention
```
nexusfeed-api-gateway:1.0.0
nexusfeed-identity-service:1.0.0
nexusfeed-post-service:1.0.0
nexusfeed-media-service:1.0.0
nexusfeed-search-service:1.0.0
```

---

## 10. Scaling Considerations

### Horizontal Scaling (Future)
Current compose file is single-instance. For scaling:

```yaml
# Multiple post-service instances
post-service:
  deploy:
    replicas: 3  # 3 replicas behind load balancer

# Load Balancer (Nginx)
nginx:
  ports:
    - "80:80"
  volumes:
    - ./nginx.conf:/etc/nginx/nginx.conf
  depends_on:
    - api-gateway
```

### Vertical Scaling
Adjust resource limits per service:
```yaml
services:
  post-service:
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 512M
        reservations:
          cpus: '0.5'
          memory: 256M
```

---

## 11. Network Security

### Internal Communication Flow
```
â”Œâ”€ Ingress (0.0.0.0:3000 â†’ Host â†’ Bridge)
â”‚
â”œâ”€ Intra-bridge communication (all services)
â”‚  â”œâ”€ Gateway â†’ Identity (JWT validation)
â”‚  â”œâ”€ Gateway â†’ Post (CRUD operations)
â”‚  â”œâ”€ Gateway â†’ Media (upload handling)
â”‚  â”œâ”€ Gateway â†’ Search (query handling)
â”‚  â””â”€ All â†’ RabbitMQ (event publishing/consuming)
â”‚
â””â”€ Egress (to Cloudinary API, external services)
```

### Future Hardening
- Network policies (if using Docker Swarm/Kubernetes)
- Certificate-based mTLS between services
- API Gateway as reverse proxy with TLS termination
- Service mesh (Istio) for production

---

## 12. Logging & Monitoring Strategy

### Log Aggregation
All containers log to STDOUT/STDERR (collected by Docker):
```bash
docker-compose logs services        # View all logs
docker-compose logs -f post-service # Follow post-service
```

### Log Driver (Default: json-file)
For production, consider:
- ELK Stack (Elasticsearch, Logstash, Kibana)
- Splunk
- CloudWatch (AWS)
- Datadog

### Environment-based Logging
```
Development: LOG_LEVEL=debug   (verbose)
Production: LOG_LEVEL=info     (standard)
```

---

## 13. Environment Configuration

### Multi-Environment Support
```
â”œâ”€â”€ .env.docker          # Development (Docker)
â”œâ”€â”€ .env.docker.staging  # Staging environment
â”œâ”€â”€ .env.docker.prod     # Production environment
```

### Secret Management (Future)
Current: `.env` files in compose
Production: 
- Docker Secrets (Swarm mode)
- AWS Secrets Manager
- HashiCorp Vault
- Kubernetes Secrets

---

## Summary: Design Principles

| Principle | Implementation |
|-----------|-----------------|
| **Modularity** | Each service independent, own container |
| **Scalability** | Stateless containers, centralized data layer |
| **Resilience** | Health checks, restart policies, message queue reliability |
| **Observability** | STDOUT logging, health endpoints, metrics ready |
| **Security** | Non-root users, network isolation, variable injection |
| **Simplicity** | Single docker-compose file, no external tools needed for dev |

---

## Next Steps

1. âœ… Create Dockerfiles for each service
2. âœ… Create docker-compose.yml orchestration
3. âœ… Create .env.docker configuration
4. ğŸ”„ Add CI/CD pipeline (GitHub Actions)
5. ğŸ”„ Deploy to cloud (AWS/GCP/Azure)
6. ğŸ”„ Add monitoring stack (Prometheus, Grafana)
7. ğŸ”„ Implement service mesh (Istio)
